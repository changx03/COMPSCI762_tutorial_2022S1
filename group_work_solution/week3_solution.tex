\documentclass[10pt]{article}

\usepackage[english]{babel}
\usepackage[linguistics]{forest}
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bookmark}
\usepackage{caption}
\usepackage{colortbl}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{lmodern}
\usepackage{mathptmx}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{svg}
\usepackage{xcolor}
\usepackage{multicol}

\usetikzlibrary{calc}

\DeclareMathOperator*{\argmax}{argmax}

\makeatletter
\def\@seccntformat#1{%
  \expandafter\ifx\csname c@#1\endcsname\c@section\else
  \csname the#1\endcsname\quad
  \fi}
\makeatother

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\sloppy

\begin{document}

\title{COMPSCI 762 2022 S1 Week 3 Solution}
\author{Luke Chang}

\maketitle

\medskip

\section{Question 1}
\label{q1}

\begin{itemize}
  \item The maximum entropy is not 1 unless it is binary. ``Entropy is 1'' does not mean anything on data with multiple classes.
  \item Decision stump (R1) provides a non-trivial baseline. Later this course, we will learn that using multiple R1 models to build high performance ensemble models.
  \item Maximum entropy indicates the outcome is not better than random guess.
  \item 0 is the minimum entropy. It indicates you are 100\% certain about the outcome.
  \item Entropy is not a probability, the range is $[0, \infty)$. It does not bound between $[0, 1]$.
\end{itemize}

\section{Question 2}
\label{q2}

\begin{itemize}
  \item The tree you draw by hand should match the plot from \texttt{sklearn}.
  \item By symmetry, ``B'' and ``C'' should have identical entropy. We only need compute one of them in order to save time.
\end{itemize}

\section{Question 3}
\label{q3}

\begin{itemize}
  \item The tree you draw by hand should match the plot from \texttt{sklearn}.
  \item Without prior condition (not looking at any attribute). There are 6 data points. 
  3 of them are positive, and others are 3 negative. 
  This is a binary case, and the outcome is 50:50. 
  Therefore, we can write $H(Y) = 1$ immediately without any computation (Max entropy). 
  \item Let consider the formula of {\em Information Gain} (IG), it is a non-negative decreasing. 
  \item The maximum IG cannot go above the parent entropy.
  \item Recall, 0 is the minimum entropy, and it indicates you are 100\% certain about the outcome.
  \item By combining two bullet points above, we know the maximum IG appears when we choose the attribute with 0 entropy 
  (all cases combine).
  \item After we found ``Shape'' maximize IG at the decision stump, we realize 
  $H(Y|\text{Color}=\text{red}, \text{Shape}=\text{triangle})$ and 
  $H(Y|\text{Color}=\text{blue}, \text{Shape}=\text{triangle})$ both have 0 entropy 
  (the output labels are 100\% certain).
  Thus, we can condition on ``Color'' without computing entropy for other attributes. 
  Other attributes can do no better than ``Color''. 
  Moreover, because we are 100\% the output when we condition on ``Color'', 
  this is the end of the decision tree. There will be no more leaf node.
\end{itemize}

\end{document}
