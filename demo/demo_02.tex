\documentclass[aspectratio=169, 10pt]{beamer}
\usetheme{Madrid}
\usefonttheme{professionalfonts}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[linguistics]{forest}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bookmark}
\usepackage{caption}
\usepackage{colortbl}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{lmodern}
\usepackage{mathptmx}
\usepackage{mathtools}
\usepackage{svg}
\usepackage{xcolor}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
}

\title{Week 2 - Tutorial 1}
\subtitle{Decision Tree}
\author{Luke Chang}
\institute{The University of Auckland}
\date{Feb, 2022}


\begin{document}

\frame{\titlepage}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Decision Tree Overview}

\begin{itemize}
    \item A decision tree models different possible decision paths, where each decision node is a conditional.
    \item Decision nodes are created based on a \textbf{splitting criterion}.
    \item Most common splitting criterions are: \textbf{Entropy} and \textbf{\href{https://scikit-learn.org/stable/modules/tree.html}{Gini}}.
    \item A tree is created recursively from the root node to the leaf nodes. 
    \item Decision tree can be used for both classification and regression.
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Entropy}

Informally, entropy measures the amount of uncertainty in a system.

\begin{block}{Shannon Entropy}
    \[
        H(X) \coloneqq - \sum_x P(X=x) \log_2 P(X=x)
    \]
\end{block}
Where $0 \log_2(0) \equiv 0$, since $\lim_{x \to 0} x \log_2(x)=0$.

\begin{itemize}
    \item<1-> What is the possible maximum entropy?
        % \onslide<2->{No upper bound}
    \item<1-> What does it mean?
        % \onslide<2->{No better than random guess.}
    \item<1-> What is the possible minimum entropy?
        % \onslide<2->{0}
    \item<1-> What does it mean?
        % \onslide<2->{You are certain about the outcome. No uncertainty.}
\end{itemize}

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}{Example: A six-sided Die}
\small
Suppose we have a standard six-sided die with each of the faces has a probability of $1/6$.

\[
    P(X=i) = \frac{1}{6}, i \in \{1,2,3,4,5,6\}
\]

Entropy:

\[
    H(X) = - \sum \frac{1}{6} \log_2 (\frac{1}{6}) = -6[\frac{1}{6} \log_2(\frac{1}{6})] = \log_2(6) \approx 2.6
\]

Suppose we have a loaded six-sided die. All 6 faces are printed ``1''.

\[ P(X=i)= 
    \begin{cases} 
        1 & \text{if } i = 1, \\
        0 & otherwise. 
    \end{cases}
\]

Entropy:

\[
    H(X) = - log_2(1) = 0
\]

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Information Gain}

\begin{block}{Information Gain}
    Information Gain = Parent Entropy - Current Conditional Entropy
    \[
        \text{IG}(Y, X) \coloneqq H(Y) - H(Y|X)
    \]
\end{block}
Where $H(Y|X)$ is the conditional entropy of the target variable $Y$ given attribute $X$ (Weighted sum):

\[
    H(Y|X) \coloneqq \sum_x P(X=x)H(Y|X=x)
\]

And $H(Y|X=x)$ is the conditional entropy of the target variable $Y$ given $X=x$:

\[
    H(Y|X=x) \coloneqq - \sum_y P(Y=y|X=x) \log_2 P(Y=y|X=x)
\]

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Estimating Probabilities}

Let $D_n$ be the subset of the data at node $n$ in the tree, we estimate the probability by counting the number of success:

\[
    P(Y=y) \coloneqq \frac{|\{i \in D_n : i_Y = y\}|}{|D_n|}
\]

Similarly:

\[
    P(X=x) \coloneqq \frac{|\{i \in D_n : i_X = x\}|}{|D_n|}
\]

Conditional probability:

\[
    P(Y=y | X=x) \coloneqq \frac{|\{i \in D_n : i_Y = y \land i_X = x\}|}{|\{i \in D_n : i_X = x \}|}
\]

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Example 1: Boolean Functions}
Give decision trees to represent the following boolean functions:
\begin{enumerate}
    \item $A \land \neg B$
\end{enumerate}

\begin{table}[]
    \begin{tabular}{lll|l}
    A & B & $\neg$B & Y \\ \hline
    0 & 0 & 1 & 0 \\
    1 & 0 & 1 & 1 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0
    \end{tabular}
\end{table}
    
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{$A \land \neg B$ Probabilities}
\small

\begin{columns}
    \begin{column}{0.5\textwidth}
        \begin{table}[]
            \begin{tabular}{lll|l}
            A & B & $\neg$B & Y \\ \hline
            0 & 0 & 1 & 0 \\
            1 & 0 & 1 & 1 \\
            0 & 1 & 0 & 0 \\
            1 & 1 & 0 & 0
            \end{tabular}
        \end{table}
    \end{column}
    \begin{column}{0.5\textwidth}
        \[ P(Y=1) = \frac{1}{4} \]
        % \[ P(A=1) = \frac{2}{4} = \frac{1}{2} \]
        % \[ P(B=1) = \frac{2}{4} = \frac{1}{2} \]
        \[ P(Y=1 | A=1) = \frac{1}{2} \]
        % \[ P(Y=0 | A=1) = \frac{1}{2} \]
        \[ P(Y=1 | A=0) = 0 \]
        % \[ P(Y=0 | A=0) = 1 \]
        \[ P(Y=1 | B=1) = 0 \]
        % \[ P(Y=0 | B=1) = 1 \]
        \[ P(Y=1 | B=0) = \frac{1}{2} \]
        % \[ P(Y=0 | B=0) = \frac{1}{2} \]
    \end{column}
\end{columns}

Root Entropy:
\[
    \begin{split}
        H(Y) & = -P(Y=1) \log_2 P(Y=1) -P(Y=0) \log_2 P(Y=0) \\
                   & = -\frac{1}{4} \log_2(\frac{1}{4}) -\frac{3}{4} \log_2(\frac{3}{4}) \\
                   & \approx 0.5 + 0.31 = 0.81
    \end{split}
\]

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
\frametitle{Entropies Condition on A}
\small

\[
    \begin{split}
        H(Y | A=1) & = -P(Y=1 | A=1) \log_2 P(Y=1 | A=1) -P(Y=0 | A=1) \log_2 P(Y=0 | A=1) \\
                   & = -\frac{1}{2} \log_2(\frac{1}{2}) -\frac{1}{2} \log_2(\frac{1}{2}) \\
                   & = 1
    \end{split}
\]

\[
    \begin{split}
        H(Y | A=0) & = -P(Y=1 | A=0) \log_2 P(Y=1 | A=0) -P(Y=0 | A=0) \log_2 P(Y=0 | A=0) \\
                   & = -\frac{0}{2} \log_2(\frac{0}{2}) -\frac{2}{2} \log_2(\frac{2}{2}) \\
                   & = 0 - 0 = 0
    \end{split}
\]

\[
    \begin{split}
        IG(Y | A) & =  H(Y) - P(A=1)H(Y | A=1) - P(A=0)H(Y | A=0)\\
                  & \approx 0.81 - \frac{2}{4} - 0 = 0.31
    \end{split}
\]

\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Entropies Condition on B}
    \small
    
    By symmetry:

    \[
        \begin{split}
            H(Y | B=1) & = -P(Y=1 | B=1) \log_2 P(Y=1 | B=1) -P(Y=0 | B=1) \log_2 P(Y=0 | B=1) \\
                       & = 0
        \end{split}
    \]
    
    \[
        \begin{split}
            H(Y | B=0) & = -P(Y=1 | B=0) \log_2 P(Y=1 | B=0) -P(Y=0 | B=0) \log_2 P(Y=0 | B=0) \\
                       & = 1
        \end{split}
    \]
    
    \[
        \begin{split}
            IG(Y | B) & =  H(Y) - P(B=1)H(Y | B=1) - P(B=0)H(Y | B=0)\\
                      & \approx 0.81 - 0 - \frac{2}{4} = 0.31
        \end{split}
    \]
    
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}
    \frametitle{Information Gain}
    \small
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \[
                \begin{split}
                    IG(Y | A) & \approx 0.31\\
                    IG(Y | B) & \approx 0.31\\
                \end{split}
            \]
            
            There is a tie on both conditions. Let's use A as root node.
        \end{column}
        \begin{column}{0.5\textwidth}
            Draw Decision Tree using \texttt{sklearn}
            \begin{figure}
                \centering
                \includegraphics[width=\columnwidth]{../plots/tree_logic_01.pdf}
            \end{figure}
        \end{column}
    \end{columns}

\end{frame}


\end{document}
